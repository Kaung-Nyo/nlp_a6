# A6 - Natural Language Processing - RAG Implementation


## Overview
This repository contains an NLP project that utilizes various libraries and frameworks to build a conversational AI model. The project is designed to demonstrate the integration of language models with vector databases for efficient retrieval and interaction.

## Directory Structure
nlp_a6/
├── app/
│    ├── main.py # Main application file to run the model    
│    └── model.py # Implementation of the chatbot model 
├── dash.Dockerfile # Dockerfile for the Dash application 
├── docker-compose.yaml # Docker Compose configuration file 
├── documents # Directory containing supporting documents 
├── figures # Directory for figures and visualizations 
├── rag-langchain.ipynb # Jupyter Notebook for experimentation 
└── vector-store # Directory for vector embeddings  

## Dependencies
- `torch`: For building and training the model.
- `langchain`: For managing conversational flows and embeddings.
- `langchain_groq`: For integrating with the Groq API.
- `InstructorEmbedding`: For embedding generation.
- `huggingface_hub`: For accessing Hugging Face models.
- `FAISS`: For efficient similarity search.


## Key Concepts
- **Retrieval-Augmented Generation (RAG)**: A technique that augments the knowledge of LLMs with additional data, allowing them to reason about topics beyond their training cutoff.

## Notebook Structure
1. **Introduction to RAG**: Explains the concept and importance of RAG in AI applications.
2. **Setup and Dependencies**: Instructions for installing necessary libraries and packages.
3. **Implementation Steps**:
   - Prompt creation
   - Data retrieval
   - Memory management
   - Chain construction for conversational flow
4. **Execution**: Running the model with example queries and observing the outputs.

## Web Application
The web application is built using Dash and is containerized using Docker. The application is designed to provide a user-friendly interface for interacting with the conversational AI model.

### Running the Application

#### Using Docker
1. Build and start the containers:
   ```bash
   docker-compose up --build
   ```

2. Access the application at:
   ```
   http://localhost:9999
   ```

#### Using Local
1. Run the application:
   ```bash
   python main.py
   ```

### Chat View
<img src="./figure/a6_web.png" width="600" length="400"/>


## Discussion
For RAG, I use two models as follow
- instructor-base(hkunlp/instructor-base) model form hugging face as retriver
- ChatGroq as a generator model

Although the retriver is instructor-base, the performance is good enough to retrive the related information about me. When I change the question, the top documents generated by the model changes and they contain the most possible answers.

For text generation, I use ChatGroq, which is based on llama-3.1-8b-instant. The model performance is really impressive and the model can generate a well-versed answers. Howerver, the answers are typically long although descriptive. I found that the bot adds a question to me in its answer. The more direct answers might be better.
